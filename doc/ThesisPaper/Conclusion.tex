\rchapter{Conclusion}

Over the course of six months a complete 
% working
gyrokinetic simulation was written. The code comprised of multiple advection steps, a poisson solver and a parallel implementation allowing the use of up to 4096 processes. This development time is much shorter than might be expected for such a code written in a lower level language such as C or Fortran and thus demonstrates the advantages of coding in python.
Unfortunately the final code did not provide a completely accurate physical model, however the results are encouraging enough to indicate that with more time for debugging an accurate model could be produced.
The resulting python code, as expected, was far too slow to have a practical use. This case was therefore a good choice of problem to test the capabilities of pyccel.

The code generated by pyccel was much better than the original python code however it was still at best three times slower than the equivalent pure Fortran implementation of the problem. This is sufficiently fast for the code to be useful however it is sub-optimal, especially in cases where the code will be reused multiple times. Simple improvements to the generated code however increased the speed so that the resulting implementation was only 1.2 times slower than the equivalent pure Fortran implementation. Further improvements of this type such as using the `elemental' keyword may provide even greater gains. Although it is a shame that these pyccel was incapable of providing these improvements, their addition was a very simple process and it is possible that they may appear in a more mature version of pyccel. This means that the results from this method are very much a valid example of how to accelerate python code in a practical manner.

The parallel performance seen in this project was also very good. The maximum number of processes is very large which is very practical for large simulations  with many points such as plasma simulations. In the tests conducted thus far the scaling seen using this method was also excellent, with 80\% efficiency being seen when 32 nodes were used. Although it is important to revisit these results when the code is running at the increased speed the first results are very promising.

The final code presented here runs sufficiently quickly when compared to the equivalent pure Fortran implementation of the problem, however there is much scope for improvement. In addition to the improvements to the Fortran code already mentioned, compiler flags could be used to improve the speed of the generated code. Algorithmic changes could also be made to improve the method used or to increase the speed by making it more specific. For example the spline implementation is very general and handles non-uniform domains, however in the case examined here the grid is uniform except near the boundaries in dimensions which do not have periodic boundary conditions. This knowledge can be exploited to increase the speed. Finally it would also be possible to pyccelise other parts of the code. For the moment only code containing pure python and numpy functions has been pyccelised, however as pyccel contains implementations for blas and MPI routines other more complicated sections could also be accelerated. This includes the spline interpolation calculation which is currently one of the main bottlenecks. Accelerating the MPI routines would also help avoid additional parallel overhead which may impact the scaling once the rest of the code is sufficiently accelerated.

In conclusion the method presented in this report generates code which is sufficiently fast to be used for real world applications, although some manual improvement is necessary to get the best possible results. In addition the parallelisation method used shows good scaling, allows for a very large number of processes and can be adapted to be used in other cases, including those where the problem has more dimensions.

