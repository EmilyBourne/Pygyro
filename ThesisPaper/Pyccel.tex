\rchapter{Acceleration with Pyccel}

\section{Introduction}

As mentioned in chapter \ref{chapter::Introduction} python runs much slower than compiled languages. Tests on the code produced thus far at first did not yield results. Once the total size of the simulation was reduced by a factor of eight, it was found that five time steps could be run on one node containing 32 processes in three hours. This means that the full simulation would take more than a year and a half to run on one node. Assuming perfect scaling, it would take nearly 5 days on the maximum number of processes (2048 processes, 128 nodes). These times are unreasonable, especially if the simulation must be rerun. It is therefore important to find a way to accelerate the code.



\section{Using Pyccel}

A function can be pyccelised if it contains only basic python and numpy methods\footnote{Not all methods are currently supported by pyccel}. As classes cannot be pyccelised, functions must be created containing the pyccelisable content. These can then be called from the class to mask the larger interface that this creates.

Once the functions to be pyccelised have been selected, the pyccelisation can begin. All functions which will be called directly should be placed in one file. All functions which are used by those functions are then placed in a second file whose name must have the form ``mod\_[XX].py'', where [XX] can be anything. The ``mod\_[XX].py'' is used by epyccel, the interactive version of pyccel, as a context, enabling it to generate a shared object file. All functions must also be given a header specifying the Fortran types of the arguments. The header can either take the form of a comment or a function decorator. Function decorators are preferable as they can be imported interactively with the function. The syntax is as follows:

\begin{lstlisting}[language=python,style=pythonStyle]
 #$ header function my_function(double,int,double[:],int[:,:])
 @types('double','int','double[:]','int[:,:]')
\end{lstlisting}

Once the functions have been assembled in the correct files and given the appropriate headers, the context must be compiled. To do this one can use the command:

\begin{lstlisting}[language=bash,style=bashStyle]
 pyccel mod_[XX].py --fflags ' -O3 -fPIC'
\end{lstlisting}

The Fortran flags must be specified to ensure that the resulting files can be converted with f2py. If one wishes to generate the pyccel file from a different folder, then this can be done with the following command

\begin{lstlisting}[language=bash,style=bashStyle]
 pyccel folder1/folder2/mod_[XX].py --output folder1/folder2   --fflags ' -O3 -fPIC'
\end{lstlisting}

This allows all files to be generated simply from one location. This is the version which is used in this project.

Alternatively if the generated Fortran code may be modified then the chosen pyccel command should be given the flag '-t'. The generated code must then be compiled. For example the following command must be used with the gfortran or intel compiler respectively:

\begin{lstlisting}[language=bash,style=bashStyle]
 gfortran -O3 -fPIC -c folder1/folder2/mod_[XX].f90 -o folder1/folder2/mod_[XX].o  -J folder1/folder2/
 ifort -O3 -fPIC -c folder1/folder2/mod_[XX].f90 -o folder1/folder2/mod_[XX].o  -module folder1/folder2/
\end{lstlisting}

Once the context file is prepared then a shared object file can be generated using epyccel. In order to do this the module to be pyccelised and the functions from the context file must be imported. A ContextPyccel object should then be created containing all the functions required, and the types of their arguments. If the pyccelisation is carried out from a folder which does not contain the modules then the position of the module in the python hierarchy must be specified using the context\_folder keyword. The following code, showing the commands required to generate the shared object for the spline module, serves as an example:

\begin{lstlisting}[language=python,style=pythonStyle]
from pyccel.epyccel import ContextPyccel
from pyccel.epyccel import epyccel

import pygyro.splines.spline_eval_funcs
from pygyro.splines.mod_context_1 import find_span,basis_funs,basis_funs_1st_der

spline_context = ContextPyccel(name='context_1', context_folder='pygyro.splines')
spline_context.insert_function(find_span, ['double[:]','int','double'])
spline_context.insert_function(basis_funs, ['double[:]','int','double','int','double[:]'])
spline_context.insert_function(basis_funs_1st_der, ['double[:]','int','double','int','double[:]'])

spline_eval_funcs = epyccel(pygyro.splines.spline_eval_funcs, context=spline_context)
\end{lstlisting}


\section{Spline Acceleration}

In order to reap maximum benefits from the acceleration it is important to accelerate the functions which take the most time. These act as a bottleneck for the program. In order to determine which functions these are, the python profiler cProfile will be used on a personal laptop to examine the performance of the programme using a grid with 10 grid points in each dimension. The results for the pure python implementation are shown in table \ref{tab::pure python profile}.

\begin{table}[ht]
 \begin{tabular}{|m{.37\textwidth}|c|c|c|c|}
  \hline
          & Total time & Number & Time & Total \\
  Function & excluding sub & of & per call & time \\
          & functions [s] & calls & [s] & [s] \\
  \hline
  \hline
  method 'Alltoall' from mpi4py & 28.745 & 250 & 0.115 & 28.745 \\
  \hline
  numpy.core.multiarray.array & 15.985 & 5143615 & 0.000 & 15.985 \\
  \hline
  bisplev from scipy & 10.089 & 906000 & 0.000 & 37.869\\
  \hline
  method scipy.interpolate. \_fitpack.\_bispev & 8.370 & 906000 & 0.000 & 8.370\\
  \hline
  atleast\_1d from numpy & 8.074 & 2630520 & 0.000 & 23.968\\
  \hline
  splev & 7.466 & 818520 & 0.000 & 19.090\\
  \hline
  reshape from numpy & 6.783 & 3065341 & 0.000 & 6.783\\
  \hline
 \end{tabular}
\caption{\label{tab::pure python profile} The results of profiling the pure python implementation}
\end{table}

We note firstly that the slowest function is mpi4pyâ€™s Alltoall function. This is due to the small size of the grid which means that calculations can run faster, and the fact that the tests were run on 3 processes. This leads to a poor data balance as 2 processes have 3x10x10x10 sized grids and the third has a 4x10x10x10 sized grid. As a collective operation Alltoall therefore is obliged to wait until all processes have reached the same point. These problems will not apply when the full simulation is run so this function will not be accelerated. 

The second bottleneck is due to internal numpy functions. There is nothing that can be done about this directly although it can be hoped that accelerating other functions will lead to less reliance on numpy which will therefore eliminate this bottleneck.

The next four bottlenecks all arise due to the implementation of the splines. Bisplev is a scipy function which calls bispev in order to evaluate a 2d spline while splev is its 1d equivalent. The reshape command is used to flatten the 2D array containing the coefficients for the bisplev function, as well as during all layout changes. The function atleast\_1d  is  only used explicitly during the initialisation, which is not profiled. This means that the function must be called implicitly,  probably by one of the scipy functions. The eval functions of the classes Spline1D and Spline2D are therefore the functions which will be accelerated first.

In order to accelerate these two functions a mod\_context\_1.py file will be created. This file will contain the functions find\_span, basis\_funs, and basis\_funs\_1st\_der. These functions are obtained from the SPL library \cite{Python SPL}.  In addition a spline\_eval\_funcs.py file will be created. The file will contain the functions eval\_spline\_1d\_scalar, eval\_spline\_1d\_vector, eval\_spline\_2d\_scalar, and eval\_spline\_2d\_cross which mimic the behaviour of the scipy functions splev and bisplev. Furthermore a function eval\_spline\_2d\_vector will be added to help avoid unaccelerated Python loops. The scipy function calls will then be replaced by an if statement which verifies whether the input has a length, and a call to the necessary function. This implementation ensures that calls elsewhere in the program do not need to be modified. It may however slow the program somewhat as it adds additional function calls which are unnecessary if the type of the data is known.

The results of the acceleration can be seen in table \ref{tab::spline profile}. It can be seen that although the spline calls remain some of the most costly function calls, the total time spent in each function is significantly reduced. For example in the case of the 2D spline evaluation, previously 37.9 seconds were spent in the function bisplev and its sub-functions, in contrast now only 11.6 seconds are spent in total. Similarly the 1D spline evaluation previously required 19.1 seconds and now takes only 6.1 seconds.

\begin{table}[ht]
\centering
 \begin{tabular}{|m{.37\textwidth}|c|c|c|c|}
  \hline
          & Total time & Number & Time & Total \\
  Function & excluding sub & of & per call & time \\
          & functions [s] & calls & [s] & [s] \\
  \hline
  \hline
  method 'Alltoall' from mpi4py & 13.469 & 250 & 0.054 & 13.469 \\
  \hline
  numpy.core.multiarray.array & 8.874 & 1471915 & 0.000 & 8.874 \\
  \hline
  eval in Spline2D & 6.758 & 906000 & 0.000 & 11.620\\
  \hline
  step in PoloidalAdvection & 5.110 & 3000 & 0.002 & 33.292\\
  \hline
  eval in Spline1D & 4.729 & 818520 & 0.000 & 6.070\\
  \hline
  step in FluxSurfaceAdvection & 4.548 & 4500 & 0.001 & 9.077\\
  \hline
  \_vectorize\_call & 4.195 & 345000 & 0.000 & 24.107\\
  \hline
 \end{tabular}
 \caption{\label{tab::spline profile} The results of profiling the implementation after the acceleration of the spline functions}
\end{table}

\section{Initialisation Acceleration}

The profiling results will once more be used to determine which functions should be accelerated. The slowest functions are the eval functions of the Spline1D and Spline2D classes, the poloidal advection and the flux surface advection. These functions call the eval functions. As part of the acceleration they can call the previously accelerated functions directly.

The advection steps use the expression for the equilibrium distribution at the boundaries. As a result, the initialisation functions must first be accelerated in order to accelerate the advection equations.

Unfortunately, unlike the splines, the initialisation cannot be accelerated while keeping the old file access patterns. The initialisation takes advantage of the flexibility of numpy which can handle both scalar and vector functions however Fortran does not have this flexibility. In order to avoid loosing speed the files will therefore be organised such that a file called mod\_initialiser\_funcs.py will be created containing all scalar functions and a file called initialiser\_func.py will be created to handle the vector case by looping over the scalar function. mod\_initialiser\_funcs.py will also be used as a context for the advection.

\begin{table}[ht]
\centering
 \begin{tabular}{|m{.37\textwidth}|c|c|c|c|}
  \hline
          & Total time & Number & Time & Total \\
  Function & excluding sub & of & per call & time \\
          & functions [s] & calls & [s] & [s] \\
  \hline
  \hline
  method 'Alltoall' from mpi4py & 26.710 & 250 & 0.107 & 26.710 \\
  \hline
  numpy.core.multiarray.array & 8.837 & 1694575 & 0.000 & 8.837 \\
  \hline
  eval in Spline2D & 6.759 & 906000 & 0.000 & 11.548\\
  \hline
  step in PoloidalAdvection & 5.157 & 3000 & 0.002 & 38.472\\
  \hline
  eval in Spline1D & 4.868 & 826360 & 0.000 & 6.223\\
  \hline
  step in FluxSurfaceAdvection & 4.550 & 4500 & 0.001 & 9.191\\
  \hline
  \_vectorize\_call & 4.225 & 345000 & 0.000 & 23.773\\
  \hline
 \end{tabular}
 \caption{\label{tab::init profile} The results of profiling the implementation after the acceleration of the spline and initialisation functions}
\end{table}

As the accelerated functions are used in the initialisation which is not profiled we do not expect to see a change in the results after these improvements. The new results of profiling are shown in table \ref{tab::init profile} and are as expected.

\section{Advection Acceleration}

We are now able to accelerate the advection functions. We begin by accelerating the poloidal advection. The files written for the splines and the initialisation are used to provide a context for the accelerated advection file. Unfortunately a context cannot have its own context. Thus a file mod\_spline\_eval\_funcs.py must be created containing the contents of mod\_context\_1.py and spline\_eval\_funcs.py.

Once this step has been accomplished successfully the pyccelisable parts of the v-parallel and flux surface advections can be added to the accelerated advection file very simply.

\begin{table}[ht]
\centering
 \begin{tabular}{|m{.37\textwidth}|c|c|c|c|}
  \hline
          & Total time & Number & Time & Total \\
  Function & excluding sub & of & per call & time \\
          & functions [s] & calls & [s] & [s] \\
  \hline
  \hline
  method 'Alltoall' from mpi4py & 3.711 & 250 & 0.015 & 3.711 \\
  \hline
  step in FluxSurfaceAdvection & 1.88 & 4500 & 0.000 & 3.469 \\
  \hline
  step in PoloidalAdvection & 1.435 & 3000 & 0.000 & 3.066\\
  \hline
  method 'solve' from scipy 'SuperLU' object & 1.353 & 88000 & 0.000 & 1.353\\
  \hline
  parallel\_gradient in ParallelGradient & 1.348 & 1000 & 0.001 & 2.608\\
  \hline
  getPerturbedRho in ParallelGradient & 1.222 & 1000 & 0.012 & 5.467\\
  \hline
  \_solve\_system\_nonperiodic in SplineInterpolator1D & 1.116 & 111330 & 0.000 & 1.116\\
  \hline
 \end{tabular}
 \caption{\label{tab::adv profile} The results of profiling the implementation after the acceleration of the spline, initialisation, and advection functions}
\end{table}

\section{Results}

\begin{table}[ht]
\centering
 \begin{tabular}{|m{.37\textwidth}|c|c|c|c|}
  \hline
          & Total time & Number & Time & Total \\
  Function & excluding sub & of & per call & time \\
          & functions [s] & calls & [s] & [s] \\
  \hline
  \hline
  step in PoloidalAdvection &  &  &  &  \\
  \hline
  step in FluxSurfaceAdvection &  &  &  &  \\
  \hline
  getPerturbedRho in ParallelGradient &  &  &  & \\
  \hline
  method 'Alltoall' from mpi4py &  &  &  & \\
  \hline
  step in VParallelAdvection &  &  &  & \\
  \hline
  method 'solve' from scipy 'SuperLU' object &  &  &  & \\
  \hline
  \_solve\_system\_nonperiodic in SplineInterpolator1D &  &  &  & \\
  \hline
 \end{tabular}
 \caption{\label{tab::draco profile} The results of profiling the implementation on the supercomputer after accelerating the functions}
\end{table}
